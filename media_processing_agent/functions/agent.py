from typing import Dict, Any, List
import os
from dotenv import load_dotenv
from datetime import datetime, timezone

import json
import logging
import asyncio
import uuid
from google.cloud.aiplatform_v1beta1.types import index_endpoint
from vertexai.generative_models import GenerativeModel, Part
from vertexai.language_models import TextEmbeddingModel
import vertexai
from google.cloud import firestore
from google.cloud.aiplatform import MatchingEngineIndex

# Load environment variables
load_dotenv()


# Get environment variables
def get_project_id():
    return os.getenv("GOOGLE_CLOUD_PROJECT", "hackason-464007")


def get_location():
    return os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")


def get_index_id():
    return os.getenv("VERTEX_AI_INDEX_ID")


def get_index_endpoint_id():
    return os.getenv("VERTEX_AI_INDEX_ENDPOINT_ID")


# Initialize services lazily
_db = None
_embedding_model = None
_vector_search_index = None


def get_firestore_client():
    global _db
    if _db is None:
        _db = firestore.Client(project=get_project_id())
    return _db


def get_embedding_model():
    global _embedding_model
    if _embedding_model is None:
        vertexai.init(project=get_project_id(), location=get_location())
        _embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-004")
    return _embedding_model


def get_vector_search_index():
    global _vector_search_index
    if _vector_search_index is None:
        index_id = get_index_id()
        if index_id:
            _vector_search_index = MatchingEngineIndex(
                index_name=f"projects/{get_project_id()}/locations/{get_location()}/indexes/{index_id}"
            )
    return _vector_search_index


logger = logging.getLogger(__name__)
MODEL_NAME = "gemini-2.5-flash"

# Child ID will be set when the agent is invoked
# Default to "demo" if not provided
CHILD_ID = "demo"


def convert_firebase_url_to_gs(firebase_url: str) -> str:
    """Convert Firebase download URL to gs:// format for better Vertex AI access"""
    try:
        # Extract bucket and object path from Firebase URL
        if "firebasestorage.app" in firebase_url and "/o/" in firebase_url:
            # Parse Firebase Storage download URL
            parts = firebase_url.split("/o/")
            if len(parts) >= 2:
                bucket_part = parts[0].split("/")[-1]  # Get bucket name
                object_part = parts[1].split("?")[0]  # Remove query parameters

                # URL decode the object path
                import urllib.parse

                object_path = urllib.parse.unquote(object_part)

                gs_url = f"gs://{bucket_part}/{object_path}"
                logger.info(f"Converted Firebase URL to gs:// format: {gs_url}")
                return gs_url
    except Exception as e:
        logger.warning(f"Failed to convert Firebase URL: {e}")

    return firebase_url  # Return original if conversion fails


def objective_analyzer(media_uri: str) -> dict:
    """Extract objective facts from media files"""
    model = GenerativeModel(MODEL_NAME)
    try:
        # Try to convert Firebase URL to gs:// format for better access
        original_uri = media_uri
        if "firebasestorage.app" in media_uri:
            media_uri = convert_firebase_url_to_gs(media_uri)

        # Determine MIME type from URL extension
        media_uri_lower = original_uri.lower()

        # Extract extension from URL (handle query parameters)
        url_path = media_uri_lower.split("?")[0]

        # Check common image formats
        if any(url_path.endswith(ext) for ext in [".jpg", ".jpeg"]):
            mime_type = "image/jpeg"
        elif url_path.endswith(".png"):
            mime_type = "image/png"
        elif url_path.endswith(".gif"):
            mime_type = "image/gif"
        elif url_path.endswith(".webp"):
            mime_type = "image/webp"
        elif url_path.endswith(".bmp"):
            mime_type = "image/bmp"
        # Check common video formats
        elif url_path.endswith(".mp4"):
            mime_type = "video/mp4"
        elif url_path.endswith(".avi"):
            mime_type = "video/x-msvideo"
        elif url_path.endswith(".mov"):
            mime_type = "video/quicktime"
        elif url_path.endswith(".webm"):
            mime_type = "video/webm"
        elif url_path.endswith(".mkv"):
            mime_type = "video/x-matroska"
        else:
            # Default to image/jpeg if can't determine
            mime_type = "image/jpeg"
            logger.warning(
                f"Could not determine MIME type from URL, defaulting to {mime_type}"
            )

        logger.info(f"Detected MIME type: {mime_type} for URL: {media_uri}")

        # Try gs:// URL first, then fallback to original URL
        try:
            media_part = Part.from_uri(uri=media_uri, mime_type=mime_type)
        except Exception as gs_error:
            logger.warning(f"gs:// URL failed, trying original URL: {gs_error}")
            media_part = Part.from_uri(uri=original_uri, mime_type=mime_type)

        prompt = """
        ã‚ãªãŸã¯ã€å­ä¾›ã®è¡Œå‹•ã‚’è¦³å¯Ÿã™ã‚‹å®¢è¦³çš„ãªåˆ†æã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚
        ã“ã®ç”»åƒ/å‹•ç”»ã‹ã‚‰è¦³å¯Ÿã§ãã‚‹å…¨ã¦ã®å®¢è¦³çš„ãªäº‹å®Ÿã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚

        ã€å³å®ˆäº‹é …ã€‘
        - è¦³å¯Ÿã•ã‚ŒãŸäº‹å®Ÿã®ã¿ã‚’è¨˜è¿°ã—ã€è§£é‡ˆã‚„æ„Ÿæƒ³ã¯å«ã‚ãªã„ã§ãã ã•ã„
        - å¹´é½¢ã‚„æœˆé½¢ã®æ¨æ¸¬ã¯è¡Œã‚ãªã„ã§ãã ã•ã„
        - JSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„

        {
            "all_observed_actions": ["è¦³å¯Ÿã•ã‚ŒãŸå…¨ã¦ã®è¡Œå‹•ã®ãƒªã‚¹ãƒˆ"],
            "observed_emotions": ["è¡¨æƒ…ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æ„Ÿæƒ…ã®ãƒªã‚¹ãƒˆ"],
            "spoken_words": ["èãå–ã‚ŒãŸç™ºè©±å†…å®¹ï¼ˆã‚ã‚‹å ´åˆï¼‰"],
            "environment": "å ´æ‰€ã‚„ç’°å¢ƒã®å®¢è¦³çš„ãªæå†™",
            "physical_interactions": ["ç‰©ç†çš„ãªç›¸äº’ä½œç”¨ï¼ˆè§¦ã‚‹ã€æŒã¤ã€æŒ‡å·®ã™ãªã©ï¼‰"],
            "body_movements": ["ä½“ã®å‹•ãï¼ˆæ­©ãã€åº§ã‚‹ã€æ‰‹ã‚’æŒ¯ã‚‹ãªã©ï¼‰"]
        }
        """

        response = model.generate_content([media_part, prompt])
        response_text = response.text.strip()

        logger.info(f"Raw response from model: {response_text[:200]}...")

        import re

        json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)
        else:
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)

        response_text = response_text.strip()

        if not response_text:
            logger.error("Empty response from model")
            return {"status": "error", "error_message": "Empty response from model"}

        try:
            facts = json.loads(response_text)
            # Add media type to facts
            facts["media_type"] = "video" if mime_type.startswith("video/") else "image"
            return {"status": "success", "report": facts}
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            logger.error(f"Response text: {response_text}")
            return {
                "status": "error",
                "error_message": f"Failed to parse JSON: {str(e)}",
            }

    except Exception as e:
        error_msg = str(e)

        # Check for URL access errors
        if (
            "Cannot fetch content from the provided URL" in error_msg
            or "URL_ERROR" in error_msg
        ):
            return {
                "status": "error",
                "error_message": "ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Firebase Storageã®URLãŒVertex AIã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                "error_details": {
                    "original_error": error_msg,
                    "solutions": [
                        "Firebase Storageã®CORSè¨­å®šã‚’ç¢ºèªã™ã‚‹",
                        "Vertex AI Service Accountã«é©åˆ‡ãªæ¨©é™ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹",
                        "ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¬é–‹ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹Cloud Storageãƒã‚±ãƒƒãƒˆã®æ¨©é™ã‚’ç¢ºèªã™ã‚‹",
                    ],
                },
            }

        return {"status": "error", "error_message": str(e)}


def perspective_determiner(facts: Dict[str, Any], child_age_months: int) -> dict:
    """Determine analysis perspectives based on media type, child age and observed facts"""
    model = GenerativeModel(MODEL_NAME)
    try:
        facts_json = json.dumps(facts, ensure_ascii=False, indent=2)
        media_type = facts.get("media_type", "image")

        if media_type == "video":
            # å‹•ç”»ã®å ´åˆï¼šå¤šè§’çš„ãªè¦–ç‚¹ï¼ˆç™ºé”ã€æ„Ÿæƒ…ã€æ€ã„å‡ºã€é¢ç™½ã„ç¬é–“ãªã©ï¼‰
            prompt = f"""
            ã‚ãªãŸã¯ã€å­ä¾›ã®æˆé•·è¨˜éŒ²ã‚’å¤šè§’çš„ã«åˆ†æã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚å‹•ç”»ã‹ã‚‰è¦³å¯Ÿã•ã‚Œã‚‹æ§˜ã€…ãªå´é¢ã‚’æ‰ãˆã¦ãã ã•ã„ã€‚

            ã€å…¥åŠ›æƒ…å ±ã€‘
            ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—: å‹•ç”»
            æœˆé½¢: {child_age_months}ãƒ¶æœˆ
            è¦³å¯Ÿã•ã‚ŒãŸäº‹å®Ÿ:
            {facts_json}

            ã€ã‚¿ã‚¹ã‚¯ã€‘
            å‹•ç”»ã‹ã‚‰è¦³å¯Ÿã•ã‚Œã‚‹å†…å®¹ã‚’åŸºã«ã€ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰æœ€ã‚‚é‡è¦ãªè¦–ç‚¹ã‚’æœ€å¤§4ã¤é¸æŠã—ã¦ãã ã•ã„ï¼š

            ã€åˆ†æã®è¦³ç‚¹ã€‘
            1. ç™ºé”ãƒ»æˆé•·ã®è¦–ç‚¹
               - è¨€èªç™ºé”ï¼ˆç™ºè©±ã€ç†è§£ï¼‰
               - é‹å‹•ç™ºé”ï¼ˆå‹•ãã€å™¨ç”¨ã•ï¼‰
               - èªçŸ¥ãƒ»ç¤¾ä¼šæ€§ã®ç™ºé”

            2. æ„Ÿæƒ…ãƒ»æ€ã„å‡ºã®è¦–ç‚¹
               - æ¥½ã—ã„ç¬é–“ã€é¢ç™½ã„ãƒã‚¤ãƒ³ãƒˆ
               - å®¶æ—ã‚„å‘¨ã‚Šã®äººã¨ã®é–¢ã‚ã‚Š
               - ç‰¹åˆ¥ãªä½“é¨“ã‚„åˆã‚ã¦ã®çµŒé¨“

            3. èµ¤ã¡ã‚ƒã‚“ç‰¹æœ‰ã®è¦–ç‚¹ï¼ˆè©²å½“ã™ã‚‹æœˆé½¢ã®å ´åˆï¼‰
               - ã‹ã‚ã„ã„ä»•è‰ã‚„ç‰¹å¾´
               - ã“ã®æ™‚æœŸãªã‚‰ã§ã¯ã®è¡Œå‹•
               - è¦ªå­ã®çµ†ã‚’æ„Ÿã˜ã‚‹ç¬é–“

            ã€å‡ºåŠ›å½¢å¼ã€‘
            {{
                "perspectives": [
                    {{
                        "type": "è¦–ç‚¹åï¼ˆdevelopment, emotional_moment, funny_point, baby_featuresç­‰ï¼‰",
                        "focus": "ã“ã®è¦–ç‚¹ã§æ³¨ç›®ã™ã¹ãå…·ä½“çš„ãªãƒã‚¤ãƒ³ãƒˆ",
                        "reason": "ãªãœã“ã®è¦–ç‚¹ãŒé‡è¦ãƒ»ç‰¹åˆ¥ãªã®ã‹",
                        "observable_signs": ["å‹•ç”»ã‹ã‚‰è¦³å¯Ÿã•ã‚ŒãŸå…·ä½“çš„ãªè¦ç´ "]
                    }}
                ],
                "analysis_note": "ã“ã®å‹•ç”»ãŒæ‰ãˆãŸç¬é–“ã®ç·åˆçš„ãªæ„å‘³"
            }}
            """
        else:
            # å†™çœŸã®å ´åˆï¼šã‚·ãƒ¼ãƒ³ç‰¹å®šã¨æ„Ÿæƒ…ãƒ»æ€ã„å‡ºã«é™å®š
            prompt = f"""
            ã‚ãªãŸã¯ã€å†™çœŸã‹ã‚‰å ´é¢ã‚„æ„Ÿæƒ…ã‚’èª­ã¿å–ã‚‹å°‚é–€å®¶ã§ã™ã€‚ã“ã®å†™çœŸãŒæ‰ãˆãŸç¬é–“ã®æ„å‘³ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

            ã€å…¥åŠ›æƒ…å ±ã€‘
            ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—: å†™çœŸ
            æœˆé½¢: {child_age_months}ãƒ¶æœˆ
            è¦³å¯Ÿã•ã‚ŒãŸäº‹å®Ÿ:
            {facts_json}

            ã€ã‚¿ã‚¹ã‚¯ã€‘
            å†™çœŸã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã‚·ãƒ¼ãƒ³ã€æ„Ÿæƒ…ã€æ€ã„å‡ºã®è¦³ç‚¹ã‹ã‚‰åˆ†æè¦–ç‚¹ã‚’æœ€å¤§4ã¤ã¾ã§é¸æŠã—ã¦ãã ã•ã„ã€‚
            â€»å†™çœŸã§ã¯ç™ºé”è©•ä¾¡ã¯è¡Œã‚ãšã€ãã®ç¬é–“ã®æƒ…æ™¯ã‚„æ„Ÿæƒ…ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚

            ã€è¦–ç‚¹é¸æŠã®æŒ‡é‡ã€‘
            1. ã‚·ãƒ¼ãƒ³ã®ç‰¹å®š
               - ã©ã‚“ãªå ´æ‰€ã‚„ã‚¤ãƒ™ãƒ³ãƒˆã‹ï¼ˆãŠç¥­ã‚Šã€å…¬åœ’ã€å®¶ãªã©ï¼‰
               - å­£ç¯€ã‚„æ™‚æœŸã®æ¨æ¸¬
               - èƒŒæ™¯ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹çŠ¶æ³

            2. æ„Ÿæƒ…ã®ç¬é–“
               - è¡¨æƒ…ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æ„Ÿæƒ…
               - ãã®ç¬é–“ã®é›°å›²æ°—
               - æ¥½ã—ã•ã‚„å–œã³ã®è¡¨ç¾

            3. æ€ã„å‡ºã¨ã—ã¦ã®ä¾¡å€¤
               - ç‰¹åˆ¥ãªä½“é¨“ã‚„åˆã‚ã¦ã®çµŒé¨“
               - å®¶æ—ã‚„å‹é”ã¨ã®é–¢ã‚ã‚Š
               - è¨˜å¿µã™ã¹ãç¬é–“

            ã€å‡ºåŠ›å½¢å¼ã€‘
            {{
                "perspectives": [
                    {{
                        "type": "è¦–ç‚¹åï¼ˆscene_context, emotional_moment, special_memoryç­‰ï¼‰",
                        "focus": "ã“ã®è¦–ç‚¹ã§æ³¨ç›®ã™ã¹ãå…·ä½“çš„ãªãƒã‚¤ãƒ³ãƒˆ",
                        "reason": "ãªãœã“ã®ç¬é–“ãŒç‰¹åˆ¥ãªã®ã‹",
                        "observable_signs": ["å†™çœŸã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å…·ä½“çš„ãªè¦ç´ "]
                    }}
                ],
                "analysis_note": "ã“ã®å†™çœŸãŒæ‰ãˆãŸç¬é–“ã®ç·åˆçš„ãªæ„å‘³"
            }}
            """

        prompt += """
        
        ã€é‡è¦ã€‘
        - è¦–ç‚¹æ•°ã¯æœ€å¤§4ã¤ã¾ã§
        - å®Ÿéš›ã«è¦³å¯Ÿã•ã‚ŒãŸå†…å®¹ã«åŸºã¥ãè¦–ç‚¹ã®ã¿ã‚’é¸æŠ
        - å„è¦–ç‚¹ã¯é‡è¤‡ã—ãªã„ã‚ˆã†ã«ç‹¬ç«‹ã—ãŸè¦³ç‚¹ã‹ã‚‰é¸ã¶
        """

        response = model.generate_content(prompt)
        response_text = response.text.strip()

        logger.info(
            f"Raw response from perspective_determiner: {response_text[:200]}..."
        )

        # Extract JSON from response
        import re

        json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)
        else:
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)

        perspectives = json.loads(response_text)
        return {"status": "success", "report": perspectives}

    except Exception as e:
        logger.error(f"Error in perspective_determiner: {e}")
        return {"status": "error", "error_message": str(e)}


def dynamic_multi_analyzer(facts: Dict[str, Any], perspective: Dict[str, Any]) -> dict:
    """Analyze facts from a specific perspective"""
    model = GenerativeModel(MODEL_NAME)
    try:
        # Validate perspective structure
        if "type" not in perspective:
            return {
                "status": "error",
                "error_message": "perspective must contain 'type' field",
            }
        if "focus" not in perspective:
            return {
                "status": "error",
                "error_message": "perspective must contain 'focus' field",
            }

        facts_json = json.dumps(facts, ensure_ascii=False, indent=2)
        media_type = facts.get("media_type", "image")

        prompt = f"""
        ã‚ãªãŸã¯ã€æŒ‡å®šã•ã‚ŒãŸè¦–ç‚¹ã‹ã‚‰å­ä¾›ã®ç¬é–“ã‚’åˆ†æã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

        ã€å…¥åŠ›æƒ…å ±ã€‘
        ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—: {media_type}
        åˆ†æè¦–ç‚¹: {perspective['type']}
        ç€ç›®ãƒã‚¤ãƒ³ãƒˆ: {perspective['focus']}
        è¦³å¯Ÿã•ã‚ŒãŸäº‹å®Ÿ:
        {facts_json}

        ã€ã‚¿ã‚¹ã‚¯ã€‘
        ä¸Šè¨˜ã®è¦–ç‚¹ã‹ã‚‰ã€è¦³å¯Ÿã•ã‚ŒãŸå†…å®¹ã‚’åˆ†æã—ã€è¦ªã«ã¨ã£ã¦ä¾¡å€¤ã®ã‚ã‚‹æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

        ã€åˆ†æã®æŒ‡é‡ã€‘
        1. å®¢è¦³çš„äº‹å®Ÿã«åŸºã¥ã„ãŸåˆ†æã‚’è¡Œã†
        2. {"å‹•ç”»ã®å ´åˆã¯ç™ºé”çš„æ„ç¾©ã‚„æˆé•·ã®æ§˜å­ã‚’å«ã‚ã‚‹" if media_type == "video" else "å†™çœŸã®å ´åˆã¯ãã®ç¬é–“ã®æƒ…æ™¯ã‚„æ„Ÿæƒ…ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"}
        3. è¦ªãŒå–œã¶ã‚ˆã†ãªæ¸©ã‹ã„è§£é‡ˆã‚’å¿ƒãŒã‘ã‚‹
        4. {"å°†æ¥ã®æˆé•·ã¸ã®æœŸå¾…ã‚’å«ã‚ã‚‹" if media_type == "video" else "æ€ã„å‡ºã¨ã—ã¦ã®ä¾¡å€¤ã‚’å¼·èª¿ã™ã‚‹"}
        5. ã‚¿ã‚°ã¯ã€Œæ¥½ã—ã„å‡ºæ¥äº‹ã€ã€Œæˆé•·ã®è¨˜éŒ²ã€ã€Œæ–°ã—ã„æŒ‘æˆ¦ã€ã€Œæ„Ÿå‹•ã®ç¬é–“ã€ãªã©ã€æ–°èè¨˜äº‹ã¨ã—ã¦å¼•ã£å¼µã‚Šã‚„ã™ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã«ã™ã‚‹

        ã€å‡ºåŠ›å½¢å¼ã€‘
        {{
            "perspective_type": "{perspective['type']}",
            "title": "ã“ã®ç¬é–“ã‚’è¡¨ã™å°è±¡çš„ãªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ15æ–‡å­—ä»¥å†…ï¼‰",
            "summary": "è¦³å¯Ÿã•ã‚ŒãŸè¡Œå‹•ã®å…·ä½“çš„ãªæå†™ã¨ã€ã“ã®è¦–ç‚¹ã§ã®æ„å‘³ï¼ˆ100æ–‡å­—ç¨‹åº¦ï¼‰",
            "significance": "ã“ã®è¦–ç‚¹ã‹ã‚‰è¦‹ãŸç™ºé”çš„é‡è¦æ€§ã‚„è¦ªã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
            "future_outlook": "ä»Šå¾Œã®æˆé•·ã§æœŸå¾…ã§ãã‚‹ã“ã¨",
            "vector_tags": ["ã‚ˆã‚Šæƒ…ç·’çš„ã§æ¤œç´¢ã—ã‚„ã™ã„ã‚¿ã‚°ã‚’5-8å€‹ç”Ÿæˆã€‚ä¾‹ï¼šã€Œåˆã‚ã¦ã§ããŸå–œã³ã®ç¬é–“ã€ã€Œå°ã•ãªæ‰‹ã§å¤§ããªæŒ‘æˆ¦ã€ã€Œç¬‘é¡”ã‚ãµã‚Œã‚‹æˆé•·ã®ä¸€æ­©ã€ã€Œè¦ªå­ã§åˆ†ã‹ã¡åˆã†é”æˆæ„Ÿã€ãªã©ã€æ„Ÿæƒ…ã¨æˆé•·ãŒä¼ã‚ã‚‹10-20æ–‡å­—ç¨‹åº¦ã®ãƒ•ãƒ¬ãƒ¼ã‚º"]
        }}

        ã€æ³¨æ„äº‹é …ã€‘
        - åŒ»å­¦çš„è¨ºæ–­ã‚„æ–­å®šçš„ãªè©•ä¾¡ã¯é¿ã‘ã‚‹
        - æ¸©ã‹ã¿ã®ã‚ã‚‹è¡¨ç¾ã‚’ä½¿ã†
        - å°‚é–€ç”¨èªã¯æœ€å°é™ã«ã—ã€åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã‚’ä½¿ã†
        """

        response = model.generate_content(prompt)
        response_text = response.text.strip()

        # Extract JSON from response
        import re

        json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
        if json_match:
            response_text = json_match.group(1)
        else:
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)

        analysis = json.loads(response_text)
        return {"status": "success", "report": analysis}

    except Exception as e:
        logger.error(f"Error in dynamic_multi_analyzer: {e}")
        return {"status": "error", "error_message": str(e)}




def generate_emotional_title(episodes: List[Dict[str, Any]]) -> str:
    """Generate an emotional title for timeline display (15-20 chars)"""
    try:
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‹ã‚‰å…·ä½“çš„ãªæƒ…å ±ã‚’åé›†
        locations = []
        actions = []
        emotions = []
        objects = []
        
        for episode in episodes:
            if isinstance(episode, dict):
                summary = episode.get("summary", "")
                tags = episode.get("tags", episode.get("vector_tags", []))
                
                # å ´æ‰€ã‚„ã‚¤ãƒ™ãƒ³ãƒˆã®æŠ½å‡º
                for place in ["å…¬åœ’", "ãŠç¥­ã‚Š", "å®¶", "ãŠã†ã¡", "å¤–", "éƒ¨å±‹", "åº­", "æµ·", "å±±", "å·"]:
                    if place in summary:
                        locations.append(place)
                
                # å…·ä½“çš„ãªè¡Œå‹•ã®æŠ½å‡º
                for action in ["éŠã¶", "ç¬‘ã†", "èµ°ã‚‹", "æ­©ã", "é£Ÿã¹ã‚‹", "é£²ã‚€", "è¦‹ã‚‹", "æŒã¤", "è§¦ã‚‹"]:
                    if action in summary:
                        actions.append(action)
                
                # æ„Ÿæƒ…ã®æŠ½å‡º
                if "ç¬‘é¡”" in summary or "æ¥½ã—ã„" in summary or "å¬‰ã—ã„" in summary:
                    emotions.append("æ¥½ã—ã„")
                if "çœŸå‰£" in summary or "é›†ä¸­" in summary:
                    emotions.append("å¤¢ä¸­")
                
                # å…·ä½“çš„ãªç‰©ã®æŠ½å‡ºï¼ˆã‚¿ã‚°ã‹ã‚‰ï¼‰
                for tag in tags:
                    # ç‰©ã‚„å…·ä½“çš„ãªè¦ç´ ã‚’å«ã‚€ã‚¿ã‚°ã‚’æ¢ã™
                    if any(item in tag for item in ["ãƒœãƒˆãƒ«", "ãŠã‚‚ã¡ã‚ƒ", "æœ¬", "ãƒœãƒ¼ãƒ«", "ã„ã¡ã”", "é£Ÿã¹ç‰©"]):
                        objects.append(tag)
        
        # ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        vertexai.init(project=get_project_id(), location=get_location())
        model = GenerativeModel(MODEL_NAME)

        
        prompt = f"""
ã“ã®å†™çœŸãŒæ‰ãˆãŸæƒ…æ™¯ã‚„ã‚·ãƒ¼ãƒ³ã‚’è¡¨ã™ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
å­ä¾›ãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ã§ã¯ãªãã€ã©ã‚“ãªæƒ…æ™¯ãƒ»é›°å›²æ°—ã®å ´é¢ãªã®ã‹ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚

ã€è¦³å¯Ÿã•ã‚ŒãŸè¦ç´ ã€‘
- å ´æ‰€: {', '.join(locations[:2]) if locations else 'æ—¥å¸¸ã®ç©ºé–“'}
- è¡Œå‹•: {', '.join(actions[:3]) if actions else 'éŠã‚“ã§ã„ã‚‹'}
- é›°å›²æ°—: {', '.join(emotions[:2]) if emotions else 'é™ã‹ãªæ™‚é–“'}
- ç‰©ã‚„ç’°å¢ƒ: {', '.join(objects[:2]) if objects else ''}

ã€ã‚¿ã‚¤ãƒˆãƒ«ã®æ–¹å‘æ€§ã€‘
- ã€Œã©ã‚“ãªã‚·ãƒ¼ãƒ³ãƒ»æƒ…æ™¯ã‹ã€ã‚’è¡¨ç¾ã™ã‚‹
- å­ä¾›ã®å‹•ä½œã‚ˆã‚Šã€ãã®å ´ã®é›°å›²æ°—ã‚„æƒ…æ™¯ã‚’æ•ãˆã‚‹
- ã‚·ãƒ³ãƒ—ãƒ«ã§ç´ ç›´ãªè¨€è‘‰ã§è¡¨ç¾

ã€è¦ä»¶ã€‘
- 20æ–‡å­—ä»¥å†…ï¼ˆçµµæ–‡å­—å«ã‚€ï¼‰
- çµµæ–‡å­—1å€‹

ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
        
        response = model.generate_content(prompt)
        emotional_title = response.text.strip()
        
        return emotional_title
        
    except Exception as e:
        logger.error(f"Failed to generate emotional title: {e}")
        return "ğŸŒˆãã‚‡ã†ã®ã§ãã”ã¨"


def save_multi_episode_analysis(
    episodes: List[Dict[str, Any]],
    media_id: str = "",
    media_source_uri: str = "",
    child_id: str = "",
    child_age_months: int = 0,
    user_id: str = "",
    captured_at: datetime = None,
) -> dict:
    """Save multiple episodes as nested array in a single media document"""
    try:
        # Generate media_id if not provided
        if not media_id:
            media_id = str(uuid.uuid4())
            logger.info(f"Generated new media_id: {media_id}")

        # Use provided child_id or default
        if not child_id:
            child_id = globals().get("CHILD_ID", "demo")

        logger.info(f"Saving {len(episodes)} episodes for media: {media_id}")

        db = get_firestore_client()

        # Prepare episodes data
        episodes_data = []
        for episode in episodes:
            # Extract episode data
            if isinstance(episode, dict) and "report" in episode:
                ep_data = episode["report"]
            else:
                ep_data = episode

            # Create abstract episode structure
            episode_entry = {
                "id": str(uuid.uuid4()),
                "type": ep_data.get("perspective_type", ep_data.get("type", "general")),
                "title": ep_data.get("title", ""),
                "summary": ep_data.get("summary", ""),
                "content": ep_data.get("content", ep_data.get("significance", "")),
                "tags": ep_data.get("vector_tags", ep_data.get("tags", [])),
                "metadata": {
                    "future_outlook": ep_data.get("future_outlook", ""),
                    "significance": ep_data.get("significance", ""),
                },
                "created_at": datetime.now(timezone.utc),
            }
            episodes_data.append(episode_entry)

        # Generate emotional title for timeline
        emotional_title = generate_emotional_title(episodes_data)

        # Save all data in single document
        media_data = {
            "media_uri": media_source_uri,
            "child_id": child_id,
            "child_age_months": child_age_months,
            "user_id": user_id,
            "emotional_title": emotional_title,  # For timeline display
            "episodes": episodes_data,
            "episode_count": len(episodes_data),
            "captured_at": captured_at if captured_at else datetime.now(timezone.utc),  # Use provided captured_at or current time
            "created_at": datetime.now(timezone.utc),
            "updated_at": datetime.now(timezone.utc),
        }

        # Save to Firestore
        media_ref = db.collection("analysis_results").document(media_id)
        media_ref.set(media_data)

        logger.info(f"âœ… Successfully saved {len(episodes_data)} episodes for media: {media_id}")

        return {
            "status": "success",
            "media_id": media_id,
            "emotional_title": emotional_title,
            "episode_count": len(episodes_data),
            "episodes": episodes_data,
        }

    except Exception as e:
        logger.error(f"âŒ Failed to save episodes: {e}")
        return {
            "status": "error",
            "error_message": f"Failed to save episodes: {str(e)}",
        }




def index_episodes(
    episodes: List[Dict[str, Any]],
    media_id: str,
    child_id: str = "",
    captured_at: datetime = None,
) -> dict:
    """Index multiple episodes for vector search"""
    try:
        # Use provided child_id or default
        if not child_id:
            child_id = globals().get("CHILD_ID", "demo")

        vector_search_index = get_vector_search_index()
        if not vector_search_index:
            logger.warning("Vector search index not configured. Skipping indexing.")
            return {"status": "skipped", "message": "Vector indexing not configured"}

        indexed_count = 0
        for episode in episodes:
            try:
                # Extract episode data
                if isinstance(episode, dict) and "report" in episode:
                    ep_data = episode["report"]
                else:
                    ep_data = episode

                episode_id = ep_data.get("id", str(uuid.uuid4()))

                # Create text for embedding - tags only
                tags = ep_data.get("tags", [])
                if not tags:
                    logger.warning(f"No tags found for episode {episode_id}, skipping indexing")
                    continue

                embedding_text = " ".join(tags)

                # Generate embeddings
                embedding_model = get_embedding_model()
                embeddings = embedding_model.get_embeddings([embedding_text])

                if embeddings and len(embeddings) > 0:
                    embedding_vector = embeddings[0].values

                    # Create datapoint
                    datapoint_id = f"{media_id}_{episode_id}"
                    restricts = [
                        {"namespace": "media_id", "allow_list": [media_id]},
                        {"namespace": "child_id", "allow_list": [child_id]},
                    ]
                    
                    # Add captured_at timestamp if provided
                    if captured_at:
                        captured_at_timestamp = int(captured_at.timestamp())
                        restricts.append(
                            {"namespace": "captured_at", "value_int": captured_at_timestamp}
                        )
                    
                    datapoint = {
                        "datapoint_id": datapoint_id,
                        "feature_vector": embedding_vector,
                        "restricts": restricts,
                    }

                    # Upsert to index
                    vector_search_index.upsert_datapoints([datapoint])
                    indexed_count += 1
                    logger.info(f"Indexed episode {episode_id}")

            except Exception as e:
                logger.error(f"Failed to index episode: {e}")
                continue

        logger.info(f"âœ… Successfully indexed {indexed_count}/{len(episodes)} episodes")
        return {
            "status": "success",
            "indexed_count": indexed_count,
            "total_episodes": len(episodes),
        }

    except Exception as e:
        logger.error(f"âŒ Failed to index episodes: {e}")
        return {
            "status": "error",
            "error_message": f"Failed to index episodes: {str(e)}",
        }








def set_child_id(child_id: str = ""):
    """Set the child ID for the current session"""
    global CHILD_ID
    CHILD_ID = child_id if child_id else "demo"
    logger.info(f"Child ID set to: {CHILD_ID}")
    return CHILD_ID




def calculate_age_months(birth_date: datetime) -> int:
    """Calculate age in months from birthdate"""
    today = datetime.now(timezone.utc)
    months = (today.year - birth_date.year) * 12 + today.month - birth_date.month
    # Adjust if the day hasn't come yet this month
    if today.day < birth_date.day:
        months -= 1
    return max(0, months)  # Ensure non-negative


def get_child_age_months(child_id: str) -> int:
    """Get child's age in months from Firestore"""
    try:
        db = get_firestore_client()
        child_doc = db.collection("children").document(child_id).get()

        if child_doc.exists:
            child_data = child_doc.to_dict()
            birth_date = child_data.get("birthDate")

            if birth_date:
                # birthDate is a Firestore Timestamp
                return calculate_age_months(birth_date)

        logger.warning(f"Could not find birthDate for child_id: {child_id}")
        return 12  # Default to 12 months

    except Exception as e:
        logger.error(f"Error getting child age: {e}")
        return 12  # Default to 12 months


def process_media_for_cloud_function(
    media_uri: str,
    user_id: str = "",
    child_id: str = "",
    child_age_months: int = None,  # Auto-calculate if not provided
    captured_at: datetime = None,  # Media capture date/time
) -> Dict[str, Any]:
    """
    Cloud Functionsã‹ã‚‰å‘¼ã³å‡ºã›ã‚‹é–¢æ•°
    ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤šè§’çš„ã«åˆ†æã—ã€è¤‡æ•°ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ä¿å­˜ã™ã‚‹
    """
    try:
        # Auto-calculate age if not provided
        if child_age_months is None and child_id:
            child_age_months = get_child_age_months(child_id)
            logger.info(
                f"Auto-calculated age for child {child_id}: {child_age_months} months"
            )
        elif child_age_months is None:
            child_age_months = 12  # Default if no child_id

        # Generate unique media ID
        media_id = str(uuid.uuid4())

        # 1. å®¢è¦³çš„äº‹å®Ÿã‚’åˆ†æ
        facts_result = objective_analyzer(media_uri)
        if facts_result.get("status") != "success":
            return facts_result

        facts = facts_result.get("report", {})

        # 2. æœˆé½¢ã«åŸºã¥ã„ã¦åˆ†æè¦–ç‚¹ã‚’æ±ºå®š
        perspectives_result = perspective_determiner(facts, child_age_months)
        if perspectives_result.get("status") != "success":
            return perspectives_result

        perspectives_data = perspectives_result.get("report", {})
        perspectives = perspectives_data.get("perspectives", [])

        if not perspectives:
            return {
                "status": "error",
                "error_message": "No perspectives determined for analysis",
            }

        logger.info(f"Determined {len(perspectives)} perspectives for analysis")

        # 3. å„è¦–ç‚¹ã‹ã‚‰ä¸¦è¡Œã—ã¦åˆ†æã‚’å®Ÿè¡Œ
        episodes = []
        for perspective in perspectives:
            analysis_result = dynamic_multi_analyzer(facts, perspective)

            if analysis_result.get("status") == "success":
                analysis_data = analysis_result.get("report", {})
                # Add perspective type to the analysis
                analysis_data["type"] = perspective["type"]
                analysis_data["perspective_type"] = perspective["type"]
                episodes.append(analysis_data)
                logger.info(f"âœ… Successfully analyzed perspective: {perspective['type']}")
            else:
                logger.error(f"âŒ Failed to analyze perspective {perspective['type']}: {analysis_result.get('error_message', 'Unknown error')}")

        if not episodes:
            return {
                "status": "error",
                "error_message": "Failed to generate any episodes",
            }

        # 4. Save all episodes in single document
        save_result = save_multi_episode_analysis(
            episodes=episodes,
            media_id=media_id,
            media_source_uri=media_uri,
            child_id=child_id,
            child_age_months=child_age_months,
            user_id=user_id,
            captured_at=captured_at,
        )

        if save_result.get("status") != "success":
            return save_result

        # 5. Index all episodes for vector search
        index_result = index_episodes(
            episodes=save_result.get("episodes", []),
            media_id=media_id,
            child_id=child_id,
            captured_at=captured_at,
        )

        # Return comprehensive result
        return {
            "status": "success",
            "media_id": media_id,
            "emotional_title": save_result.get("emotional_title", ""),
            "child_age_months": child_age_months,
            "episode_count": len(episodes),
            "indexed_count": index_result.get("indexed_count", 0),
            "perspectives": [ep["type"] for ep in episodes],
            "analysis_note": perspectives_data.get("analysis_note", ""),
        }

    except Exception as e:
        logger.error(f"Error processing media: {str(e)}")
        return {"status": "error", "error_message": str(e)}


# Cloud Functions ã§ã¯ ADK Agent ã¯ä½¿ç”¨ã—ãªã„
